(window.webpackJsonp=window.webpackJsonp||[]).push([[60],{175:function(t,n,e){"use strict";e.r(n);var r=e(0),s=Object(r.a)({},function(){var t=this,n=t.$createElement,e=t._self._c||n;return e("div",{staticClass:"content"},[t._m(0),t._v(" "),e("p",[t._v("1.IDE:集成开发环境\n编写、调试、发布Python程序的工具\n文本工具类IDE\nIDLE\nNotepad++\nSublime Text\nVim & Emacs\nAtom\nKomodo Edit\n集成工具类IDE\nPyCharm\nWing\nPyDev & Eclipse\nVisual Studio\nAnaconda & Spyder\nCanopy")]),t._v(" "),e("p",[t._v("2.IDLE\n自带、默认、常用、入门级\n适用于\n-Python入门\n-功能简单直接\n300+代码以内")]),t._v(" "),e("p",[t._v("3.Sublime Text\n-专为程序员开发的第三方专用编程工具\n-专业编程体验\n-多种编程风格\n-工具非注册免费使用")]),t._v(" "),e("p",[t._v("4.Wing\n-公司维护，工具收费\n-调试功能丰富\n-版本控制，版本同步\n-适合多人共同开发")]),t._v(" "),e("p",[t._v("5.Visual Studio & PTVS\n-微软公司维护\n-Win环境为主\n-调试功能丰富")]),t._v(" "),e("p",[t._v("6.Eclipse\nPyDev\n开源IDE开发工具\n需要有一定开发经验")]),t._v(" "),e("p",[t._v("7.PyCharm\n-社区版免费\n-简单，集成度高\n-适合较复杂工程")]),t._v(" "),e("p",[t._v("8.Canopy\n科学计算，数据分析\n公司维护，工具收费\n支持近500个第三方库\n适合科学计算领域应用开发")]),t._v(" "),e("p",[t._v("9.Anaconda\n科学计算，数据分析\n开源免费\n支持近800个第三方库")]),t._v(" "),e("p",[t._v("#网络爬虫之规则\n#Requests库入门")]),t._v(" "),t._m(1),t._v(" "),e("p",[t._v("1.官方文档\nhttp://www.python-requests.org")]),t._v(" "),e("p",[t._v("2.pip install requests")]),t._v(" "),t._m(2),e("p",[t._v("3.Requests库的7个主要方法\nrequests.request()构造一个请求，支撑以下各方法的基础方法\nrequests.get()\n获取HTML网页的主要方法，对应于HTTP的GET\nrequests.head()\n获取HTML网页头信息的方法，对应于HTTP的HEAD\nrequests.post()\n向HTML网页提交POST请求的方法，对应于HTTP的POST\nrequest.put()\n向HTML网页提交PUT请求的方法，对应于HTTP的PUT\nrequest.patch()\n向HTML网页提交局部修改请求，对应于HTTP的PATCH\nrequest.delete()\n向HTML网页提交删除请求，对应于HTTP的DELETE")]),t._v(" "),t._m(3),t._v(" "),e("p",[t._v('1.HTTP协议\nHTTP，Hypertext Transfer Protocol，超文本传输协议\nHTTP是一个基于"请求与响应"模式的、无状态的应用层协议\nHTTP协议采用URL作为定位网络资源的标识\nURL格式 http://host[:port][path]\nhost:合法的Internet主机域名或IP地址\nport:端口号，缺省端口为80\npath:请求资源的路径\nURL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源。')]),t._v(" "),e("p",[t._v("2.HTTP协议对资源的操作\nGET：请求获取URL位置的资源\nHEAD：请求获取URL位置资源的响应消息报告，即获得该资源的头部信息\nPOST：请求向URL位置的资源后附加新的数据\nPUT：请求向URL位置存储一个资源，覆盖原URL位置的资源\nPATCH：请求局部更新URL位置的资源，即改变该处资源的部分类容\nDELETE：请求删除URL位置存储的位置")]),t._v(" "),e("p",[t._v("3.理解PATCH和PUT的区别\n假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段\n需求：用户修改了UserName，其他不变。\n-采用PATCH，仅向URL提交UserName的局部更新请求。\n-采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除。\nPATCH的最主要好处：节省网络带宽")]),t._v(" "),e("p",[t._v("4.HTTP协议与Requests库功能一致性一致\nRequests库的head()方法\nr=requests.head('http://httpbin.org/get')\nr.headers\nr.text\nRequests库的post()方法\npayload={'key1':'value1','key2':'value2'}\nr=requests.post('http://httpbin.org/post',data=payload)\nprint(r.text)\n{...\n'form':{\n'key2':'value2'\n'key1':'value1'\n},\n}\n向URL POST一个字典，自动编码为form(表单)\nRequests库的post()方法\nr=requests.post(\"http://httpbin.org/post\",data='abc')\nprint(r.text)\n{\n'data':'abc'\n'form':{\n}\n}\n向URL POS一个字符串，自动编码为data\nRequests库的put()方法\npayload={'key1':'value1','key2':'value2'}\nr=requests.put('http://httpbin.org/put',data=payload)\nprint(t.text)\n{...\n'form':{\n'key2':'value2',\n'key1':'value1'\t\n}\n}")]),t._v(" "),t._m(4),t._v(" "),e("p",[t._v("1.requests.request(method,url,**kwargs)\nmethod:请求方式，对应get/put/post等7种\nurl：拟获取页面的url链接\n**kwargs:控制访问的参数，共13个\nrequests.request(method,url,**kwargs)")]),t._v(" "),e("p",[t._v("2.method:请求方式\nr=requests.request('GET',url,**kwargs)\nr=requests.request('HEAD',url,**kwargs)\nr=requests.request('POST',url,**kwargs)\nr=requests.request('PUT',url,**kwargs)\nr=requests.request('PATCH',url,**kwargs)\nr=requests.request('delete',url,**kwargs)\nr=requests.request('OPTIONS',url,**kwargs)")]),t._v(" "),e("p",[t._v("3.**kwargs:控制访问的参数，均为可选项\nparams:字典或字节序列，作为参数增加到url中")]),t._v(" "),t._m(5),t._v(" "),e("p",[t._v("4.requests.get(url,params=None,**kwargs)\nurl:拟获取页面的url链接\nparams:url中的额外参数，字典或字节流格式，可选\n**kwargs:12个控制访问的参数\nrequests.head(url,**kwargs)\nurl:拟获取页面的url链接\n**kwargs:13个控制访问的参数\nrequests.post(url,data=None,json=None,**kwargs)\nurl:拟更新页面的url链接\ndata:字典、字节序列或文件，Request的内容\njson：JSON格式的数据,Request的内容\n**kwargs:11个控制访问的参数\nrequest.put(url,data=None,**kwargs)\nurl:拟更新页面的url链接\ndata:字典、字节序列或文件，Request的内容\n**kwargs:12个控制访问的参数\nrequest.patch(url,data=None,**kwargs)\nurl:拟更新页面的url链接\ndata:字典、字节序列或文件，Request的内容\n**kwargs:12个控制访问的参数\nrequest.delete(url,**kwargs)\nurl:拟删除页面的url链接\n**kwargs:13个控制访问的参数")]),t._v(" "),t._m(6),t._v(" "),e("p",[t._v("1.r=requests.get(url)\nRequest:构造一个向服务器请求资源的Request对象\nResponse:返回一个包含服务器资源的Response对象\nResponse对象包含爬虫返回的内容")]),t._v(" "),t._m(7),t._v(" "),e("p",[t._v("2.Response对象的属性\nr.status_code:HTTP请求的返回状态，200表示连接成功，404表示失败\nr.text:HTTP响应内容的字符串形式，即url对应的页面内容\nr.encoding:从HTTP header中猜测的响应内容编码方式\nr.apparent_encoding:从内容中分析出的响应内容编码方式（备选编码方式）\nr.content:HTTP响应内容的二进制形式")]),t._v(" "),e("p",[t._v("3.理解Response的编码\nr.encoding:如果header中不存在charset字段，则认为编码为ISO-8859-1\nr.apparent_encoding:根据网页内容分析出的编码方式")]),t._v(" "),t._m(8),t._v(" "),e("p",[t._v("1.理解Requests库的异常\nrequests.ConnectionError:网络连接错误异常，如DNS查询失败，拒绝连接等\nrequests.HTTPError:HTTP错误异常\nrequests.URLRequired:URL缺失异常\nrequests.TooManyRedirects:超过最大重定向次数，产生重定向异常\nrequests.ConnectTimeout:连接远程服务器超时异常\nrequests.Timeout:请求URL超时，产生超时异常\nr.raise_for_status():如果不是200，产生异常requests.HTTPError")]),t._v(" "),e("p",[t._v("2.爬取网页的通用代码框架")]),t._v(" "),t._m(9),e("p",[t._v("#网络爬虫的“盗亦有道”")]),t._v(" "),e("p",[t._v("##网络爬虫引发的问题")]),t._v(" "),e("p",[t._v("1.网络爬虫的尺寸\n爬取网页，玩转网页：小规模，数据量小，爬取速度不敏感，>90%，Requests库\n爬取网站，爬取系列网站：中规模，数据规模较大，爬取速度敏感，Scrapy库\n爬取全网：大规模，搜索引擎，爬取速度关键，定制开发")]),t._v(" "),e("p",[t._v("2.网络爬虫引发的问题\n-网络爬虫的“骚扰”\n受限于编写水平和目的，网络爬虫将会为web服务器带来巨大的资源开销\n-网络爬虫的法律风险\n服务器上的数据有产权归属\n网络爬虫获取数据后牟利将带来法律风险\n-网络爬虫泄漏隐私\n网络爬虫可能具备突破简单访问控制的能力，获得被保护数据从而泄露个人隐私。")]),t._v(" "),e("p",[t._v("3.网络爬虫的限制\n-来源审查：判断User-Agent进行限制\n检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问。\n-发布公告：Robots协议\n告知所有爬虫网站的爬取策略，要求爬虫遵守")]),t._v(" "),e("p",[t._v("##Robots协议")]),t._v(" "),e("p",[t._v("1.Robots协议\nRobots Exclusion Standard 网络爬虫排除标准\n作用：网站告知网络爬虫那些页面可以抓取，哪些不行。\n形式：在网站根目录下的robots.txt文件")]),t._v(" "),t._m(10),t._v(" "),e("p",[t._v("3.案例\nhttp://www.baidu.com/robots.txt\nhttp://www.qq.com/robots.txt\nhttp://news.qq.com/robots.txt\nhttp://www.moe.edu.cn/robots.txt(无robots协议)")]),t._v(" "),e("p",[t._v("4.Robots.txt文件要放在网站的根目录\n并不是所有的网站都有Robots协议\n没有robots.txt文件，允许所有爬虫无限制地爬取其内容")]),t._v(" "),e("p",[t._v("##Robots协议的遵守方式")]),t._v(" "),e("p",[t._v("1.Robots协议的使用\n网络爬虫：自动或人工识别robots.txt，再进行内容爬取。\n约束性：Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险")]),t._v(" "),e("p",[t._v("2.对Robots协议的理解\n爬取全网：必须遵守\n爬取网站，爬取系列网站：非商业且偶尔，建议遵守；商业利益：必须遵守\n爬取网页，玩转网页：访问量较大，建议遵守；访问量很小：可以遵守\n类人行为可不参考Robots协议")]),t._v(" "),e("p",[t._v("#Requests库网络爬虫实战（5个实例）")]),t._v(" "),e("p",[t._v("##京东商品页面的爬取")]),t._v(" "),t._m(11),e("p",[t._v("##亚马逊商品页面的爬取")]),t._v(" "),t._m(12),e("p",[t._v("##百度360搜索关键词提交")]),t._v(" "),t._m(13),e("p",[t._v("##网络图片的爬取和存储")]),t._v(" "),t._m(14),e("p",[t._v("##IP地址归属地的自动查询")]),t._v(" "),t._m(15),e("p",[t._v("#网络爬虫之提取")]),t._v(" "),e("p",[t._v("#Beautiful Soup库入门\n##Beautiful Soup库的安装")]),t._v(" "),e("p",[t._v("1.pip install beautifulsoup4")]),t._v(" "),e("p",[t._v("2.import requests\nfrom bs4 import BeautifulSoup\nr=requests.get('http://python123.io/ws/demo.html')\ndemo=r.text\nsoup=BeautifulSoup(demo,'html.parser')\nprint(soup.prettify())")]),t._v(" "),e("p",[t._v("3.from bs4 import BeautifulSoup\nsoup=BeautifulSoup('")]),e("p",[t._v("data")]),e("p",[t._v("','html.parser')")]),t._v(" "),e("p",[t._v("##Beautiful Soup库的基本元素")]),t._v(" "),e("p",[t._v('1.Beautiful Soup库是解析、遍历、维护"标签树"的功能库')]),t._v(" "),e("p",[t._v("2.Beautiful Soup库的引用\nBeautiful Soup库，也叫beautifulsoup4或bs4\nfrom bs4 import BeautifulSoup\nimport bs4")]),t._v(" "),e("p",[t._v("3.HTML/XML文档<->标签树<->BeautifulSoup类")]),t._v(" "),t._m(16),t._v(" "),e("p",[t._v("4.Beautiful Soup库解析器\nbs4的HTML解析器，BeautifulSoup(mk,'html.parser'),安装bs4库\nlxml的HTML解析器，BeautifulSoup(mk,'lxml'),pip install lxml\nlxml的XML解析器，BeautifulSoup(mk,'xml'),pip install lxml\nhtml5lib的解析器,BeautifulSoup(mk,'html5lib'),pip install html5lib")]),t._v(" "),e("p",[t._v("5.Beautiful Soup类的基本元素\nTag，标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾\nName，标签的名字，")]),e("p",[t._v("...")]),t._v("的名字的是'p'，格式是"),e("tag",[t._v(".name\nAttributes，标签的属性，字典形式组织，格式:"),e("tag",[t._v(".attrs\nNavigableString，标签内非属性字符串，<>...<>中字符串，格式："),e("tag",[t._v(".string\nComment：标签内字符串的注释部分，一种特殊的Comment类型"),e("p"),t._v(" "),e("p",[t._v("6.当HTML文档中存在多个相同Tag标签是，Soup."),e("tag",[t._v("是返回其中的第一个")])],1),t._v(" "),e("p",[t._v("7.type(tag.attrs)\n<class 'dict'>\ntype(tag)\n<class 'bs4.element.Tag'>\n标签可以有0个或多个属性，如果没有属性，则tag.attrs获得空字典\ntype(soup.p.string)\n<class 'bs4.element.NavigableString'>\nNavigableString是可以跨越多个标签层次的")]),t._v(" "),e("p",[t._v("8.newsoup=BeautfulSoup('"),e("b")]),e("p",[t._v("This is not a comment")]),t._v("','html.parser')"),e("p"),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("newsoup.b.string\n'this is a comment'")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("type(newsoup.b.string)\n<class 'bs4.element.Comment'>")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("newsoup.p.string\n'this is not a comment'")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("type(newsoup.p.string)\n<class 'bs4.element.NavigableString'>")])])])]),t._v(" "),e("p",[t._v("##基于bs4库的HTML格式化和编码")]),t._v(" "),e("p",[t._v("1.让")]),e("html",[t._v("内容更加友好的显示\nbs4的prettify()方法：为HTML文档的标签和文档增加换行符"),e("p"),t._v(" "),e("p",[t._v("2.bs4库将任何读入的HTML文件或字符串都转换成UTF-8编码")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("print(soup.p.prettify())\n")]),e("p",[t._v("\n中文\n")]),e("p")])])]),t._v(" "),e("p",[t._v("##基于bs4库的HTML内容遍历方法")]),t._v(" "),e("p",[t._v("1.标签树的下行遍历\n.contents：子节点的列表，将"),e("tag",[t._v("所有儿子节点存入列表\n。children：子节点的迭代类型，与.contents类似，用于循环遍历儿子节点\n.descendants：子孙节点的迭代类型，包含所有子孙节点，用于循环遍历\nfor child in soup.bady.children:\nprint(child)\n#遍历儿子节点\nfor child in soup.body.descendants:\nprint(child)\n#遍历子孙节点")])],1),t._v(" "),e("p",[t._v("2.标签树的上行标签\n.parent：节点的父亲标签\n.parents：节点先辈标签的迭代类型，用于循环遍历先辈节点")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("soup=BeautifulSoup(\"damo\",'parser')\nfor parent in soup.a.parents:\nif parent is None:\nprint(parent)\nelse:\nprint(parent.name)\np\nbody\nhtml\n[document]")])])])]),t._v(" "),e("p",[t._v("3.标签树的平行遍历\n.next_sibling：返回按照HTML文本顺序的下一个平行节点标签\n.previous_sibling：返回按照HTML文本顺序的上一个平行节点标签\n.next_siblings：迭代类型，返回按照HTML文本顺序的后续所有平行节点标签\n.previous_siblings：迭代类型，返回按照HTML文本顺序的前续所有平行节点标签\n平行遍历发生在同一个父节点下的各节点间\nfor sibling in soup.a.next_siblings:\nprint(sibling)\n#遍历后续节点\nfor sibling in soup.a.previsous_siblings:\nprint(sibling)\n#遍历前续节点")]),t._v(" "),e("p",[t._v("#信息组织与提取方法\n##基于bs4库的HTML内容查找方法")]),t._v(" "),e("p",[t._v("1.<>.find_all(name,attrs,recursive,string,**kwargs)\n返回一个列表类型，存储查找的结果\nname:对标签名称的检索字符串\nattrs:对标签属性值的检索字符串，可标注属性检索\nrecursive:是否对子孙全部检索，默认True\nstring:<>...</>中字符串区域的检索字符串")]),t._v(" "),e("p",[t._v("2.-soup.find_all('a')")]),t._v(" "),e("p",[t._v("-soup.find_all(['a','b'])\n-for tag in soup.find_all(True):\nprint(tag.name)\nhtml\nhead\ntitle\nbody\np\nb\np\na\na\n-import re\nfor tag in soup.find_all(re.compile('b')):\nprint(tag.name)\nbody\nb")]),t._v(" "),e("p",[t._v("3.-soup.find_all('p','course')\n-soup.find_all(id='link1')\n-soup.find_all(id='link')\n-import re\nsoup.find_all(id=re.compile('link'))")]),t._v(" "),e("p",[t._v("4.-soup.find_all('a')\n-soup.find_all('a','recursive'=False)")]),t._v(" "),e("p",[t._v("5.-soup.find_all(string='Basic python')\n-import re\nsoup.find_all(string=re.compile('python'))")]),t._v(" "),e("p",[t._v("6."),e("tag",[t._v("(..)等价于"),e("tag",[t._v(".find_all(..)\nsoup(..)等价于soup.find_all(..)")])],1)],1),t._v(" "),e("p",[t._v("7.扩展方法\n<>.find:搜索且只返回一个结果，字符串类型，同.find_all()参数\n<>.find_parents()：在先辈节点搜索，返回列表类型，同.find_all()参数\n<>.find_parent()：在先辈节点中返回一个结果，字符串类型，同.find_all()参数\n<>.find_next_siblings()：在后续平行节点中搜索，返回列表类型，同.find_all()参数\n<>.find_next_sibling():在后续平行节点中返回一个结果，字符串类型，同.find()参数\n<>.find_previous_siblings()：在前序平行节点中搜索，返回列表类型，同.find_all()参数\n<>.find_previous_sibling()：在前序平行节点中返回一个结果，字符串类型，同.find()参数")]),t._v(" "),e("p",[t._v("##信息标记的三种形式")]),t._v(" "),e("p",[t._v("1.信息的标记\n-标记后的信息可形成信息组织结构，增加了信息维度\n-标记后的信息可用于通信、存储或展示\n-标记后的结构与信息一样具有重要价值\n-标记后的信息更利于程序理解和运用")]),t._v(" "),e("p",[t._v("2.HTML的信息标记\nHTML超文本传输协议\nHTML是WWW（World Wide Web）的信息组织方式\n超文本：声音图像视频\n文本\nHTML通过预定义的<>..</>标签形式组织不同类型的信息")]),t._v(" "),e("p",[t._v("3.信息标记的三种形式\nXML、JSON、YAML")]),t._v(" "),e("p",[t._v("4.XML\neXtensible Markup Language\n"),e("img",{attrs:{src:"china.jpg",size:"10"}}),t._v(".."),t._v("\n标签Tag 名称Name 属性Attribute\n"),e("img",{attrs:{src:"china.jpg",size:"10"}}),t._v("\n空元素的缩写形式\n"),t._v("\n注释书写形式")]),t._v(" "),e("p",[t._v("5.JSON\nJavaScript Object Notation\n有类型的键值对 key:value\n\"name\":\"北京理工大学\"\n\"\"类型\n键key\n值value\n\"name\":['北京理工大学',\"延安自然科学院\"]\n多值用[,]组织\n键值对嵌套用{,}\n\"name\":{\n'newName':'北京理工大学',\n'oldName':'延安自然科学院'\n}\n'key':'value'\n'key':['value1','value2']\n'key':{\n'subkey1':'subvalue1'\n'subkey2':'subvalue2'\n}")]),t._v(" "),e("p",[t._v("6.YAML\nYAML Ain't Markup Language\n无类型键值对key:value\nname:北京理工大学\n键key:值value\n仅字符串\nname:\nnewName:北京理工大学\noldName:延安自然科学院\n表达并列关系\nname:\n-北京理工大学\n-延安自然科学院\n|表达整块数据 #表示注释\ntext:|  #学校介绍\n北京理工大学创立于1940年，前身是延安自然科学院，..\n....学校现隶属于工业和信息化部\nkey:value\nkey:#Comment\n-value1\n-value2\nkey:\nsubkey1:subvalue1\nsubkey2:subvalue2")]),t._v(" "),e("p",[t._v("##三种信息标记形式的比较")]),t._v(" "),e("p",[t._v("1.三种信息标记形式的比较\nXML 最早的通用信息标记语言，可扩展性好，但繁琐\nJSON 信息有类型，适合程序处理(js)，较XML简洁\nYAML 信息无类型，文本信息比例最高，可读性好")]),t._v(" "),e("p",[t._v("2.三种信息标记形式的比较\nXML Internet上的信息交互与传递\nJSON 移动应用云端和节点的信息通信，一般用于程序对接口处理的地方，无注释\nYAML 各类系统的配置文件，有注释易读")]),t._v(" "),e("p",[t._v("##信息提取的一般方法")]),t._v(" "),e("p",[t._v("1.信息提取的一般方法\n方法一：完整解析信息的标记形式，再提取关键信息\nXML JSON YAML\n需要标记解析器 例如：bs4库的标签树遍历\n优点：信息解析准确\n缺点：提取过程繁琐，速度慢")]),t._v(" "),e("p",[t._v("2.方法二：无视标记形式，直接搜索关键信息\n搜索\n对信息的文本查找函数即可\n优点：提取过程简洁，速度较快。\n缺点：提取结果准确性与信息内容相关。")]),t._v(" "),e("p",[t._v("3.融合方法：结合形式解析与搜索方法，提取关键信息。\nXML JSON YAML 搜索\n需要标记解析器及文本查找函数")]),t._v(" "),e("p",[t._v("4.实例\n提取HTML中所有URL链接\n思路：1、搜索到所有"),e("a",[t._v("标签\n2、解析"),e("a",[t._v("标签格式，提取href后的链接内容")])])]),t._v(" "),e("p",[t._v("5.>>>from bs4 import BeautifulSoup")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("soup=BeautifulSoup(demo,'html.parser')\nfor link in soup.find_all('a'):\nprint(link.get('href'))\nhttp://www.icourse163.org/course/BIT-268001\nhttp://www.icourse163.org/course/BIT-1001870001")])])])]),t._v(" "),e("p",[t._v('#实例1：中国大学排名爬虫\n##"中国大学排名定向爬虫"实例介绍')]),t._v(" "),e("p",[t._v("1.功能描述\n输入：大学排名URL链接\n输出：大学排名信息的屏幕输出（排名，大学名称，总分）\n技术路线：requests-bs4\n定向爬虫：仅对输入URL进行爬取，不扩展爬取")]),t._v(" "),e("p",[t._v("2.程序的结构设计\n步骤1：从网络上获取大学排名网页内容\ngetHTMLText()\n步骤2：提取网页内容中信息到合适的数据结构\nfillUnivList()\n步骤3：利用数据结构展示并输出结构\nprintUnivList()")]),t._v(" "),e("p",[t._v('##"中国大学排名定向爬虫"实例编写')]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("import requests\nfrom bs4 import BeautifulSoup\nimport bs4\n\ndef getHTMLText(url):\n\ttry:\n\t\tr=requests.get(url,timeout=30)\n\t\tr.raise_for_status()\n\t\tr.encoding=r.apparent_encoding\n\t\treturn t.text\n\texcept:\n\t\treturn ''\n\ndef fillUnivList(ulist,html):\n\tsoup=BeautifulSoup(html,'html.parser')\n\tfor tr in soup.find('tbody').children:\n\t\tif isinstance(tr,bs4.element.Tag):\n\t\t\ttds=tr('td')\n\t\t\tulist.append([tds[0].string,tds[1].string,tds[2].string])\n\ndef printUnivList(ulist,num):\n\tprint(\"{:^10}\\t{:^6}\\t{:^10}\".format(\"排名\",\"学校名称\",\"总分\"))\n\tfor i in range(num):\n\t\tu=ulist[i]\n\t\tprint(\"{:^10}\\t{:^6}\\t{:^10}\".format(u[0]),u[1],u[2])\n\ndef main():\n\tuinfo=[]\n\turl='httP://www.zuihaodaxue.cn/zuihaodaxuepaimi.html'\n\thtml=getHTMLText(url)\n\tfillUnivList(unifo,html)\n\tprintUnivList(unifo,20) #20 univs\nmain()\n\n")])])]),e("p",[t._v('##"中国大学排名定向爬虫"实例优化')]),t._v(" "),e("p",[t._v("1.中文对齐问题的原因\n当中文字符宽度不够时，采用西文字符填充；中西文字符占用宽度不同。")]),t._v(" "),e("p",[t._v("2.中文对齐问题的解决\n采用中文字符的空格填充chr(12288)")]),t._v(" "),e("ol",{attrs:{start:"3"}},[e("li")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v('def printUnivList(ulist,num):\n\ttplt=\'{:^10}\\t{:{3}^10}\\t{:^10}\n\tprint(tplt.format("排名","学校名称","总分",chr(12288)))\n\tfor i in range(num):\n\t\tu=ulist[i]\n\t\tprint(tplt.format(u[0]),u[1],u[2],chr(12288))\n')])])]),e("p",[t._v("#网络爬虫之实战")]),t._v(" "),e("p",[t._v("#Re(正则表达式)库入门\n##正则表达式的概念")]),t._v(" "),e("p",[t._v("1.正则表达式\nregular expression regex RE\n正则表达式是用来简洁表达一组字符串的表达式\n正则表达式优势：简洁‘一行胜千言’")]),t._v(" "),e("p",[t._v("2.正则表达式\n通用的字符串表达框架\n简洁表达一组字符串的表达式\n针对字符串表达‘简洁’和‘特征’思想的工具\n判断某字符串的特征归属")]),t._v(" "),e("p",[t._v("3.正则表达式在文本处理中十分常用\n-表达文本类型的特征(病毒、入侵等)\n-同时查找或特换一组字符串\n-匹配字符串的全部或部分\n正则表达式主要应用在字符串匹配中")]),t._v(" "),e("p",[t._v("4.正则表达式的使用\n编译：将符合正则表达式语法的字符串转换成正则表达式特征\nregex='P(Y|YT|YTH|YTHO)?N'\n编译：p=re.compile(regex)->特征")]),t._v(" "),e("p",[t._v("##正则表达式的语法")]),t._v(" "),e("p",[t._v("1.正则表达式的语法\n正则表达式语法由字符和操作符构成")]),t._v(" "),e("p",[t._v("2.正则表达式的常用操作符\n.表示任何单个字符\n[]字符集，对单个字符给出取值范围，[abc]表示a,b,c,[a-z]表示a到z单个字符\n[^]非字符集，对单个字符给出排除范围，[^abc]表示非a或b或c的单个字符\n"),e("em",[t._v("前一个字符0次或无限次扩展，abc")]),t._v("表示ab、abc、abcc、abccc等\n+前一个字符1次或无限次扩展，abc+表示abc、abcc、abccc等\n?前一个字符0次或1次扩展，abc?表示ab、abc\n|左右表达式任意一个，abc|def表示abc、def\n{m}扩展前一个字符m次，ab{2}c表示abbc\n{m,n}扩展前一个字符m至n次(含n)，ab{1,2}c表示abc,abcc\n^匹配字符串开头，^abc表示abc且在一个字符串的开头\n$匹配字符串结尾，abc$表示abc且在一个字符串的结尾\n()分组标记，内部只能使用|操作符，(abc|def)表示abc、def\n\\d数字，等价于[0-9]\n\\w单词字符，等价于[A-Za-z0-9]")]),t._v(" "),e("p",[t._v("3.经典正则表达式实例\n^[A-Za-z]+$ 由26个字母组成的字符串\n^[A-Za-z0-9]+$ 由26个字母和数字组成的字符串\n^-?\\d+$ 整数形式的字符串\n^[0-9]"),e("em",[t._v("[1-9][0-9]")]),t._v("$ 正整数形式的字符串\n[1-9]\\d{5} 中国境内邮政编码，6位\n[\\u4e00-\\u9fa5] 匹配中文字符\n\\d{3}-\\d{8}|\\d{4}-\\d{7} 国内电话号码，010-68913536")]),t._v(" "),e("p",[t._v("4.匹配IP地址的正则表达式\nIP地址字符串形式的正则表达式(IP地址分4段，每段0-255)\n0-99:[1-9]?\\d\n100-199:1\\d{2}\n200-249:2[0-4]\\d\n250-255:25[0-5]\n(([1-9]?\\d|1\\d{2}|2[0-4]\\d|25[0-5]).){3}([1-9]?\\d|1\\d{2}|2[0-4]\\d|25[0-5])")]),t._v(" "),e("p",[t._v("##Re库的基本使用")]),t._v(" "),e("p",[t._v("1.Re库介绍\nRe库是Python的标准库，主要用于字符串匹配\n调用方式：import re")]),t._v(" "),e("p",[t._v("2.正则表达式的表示类型\n-raw string类型（原生字符串类型）\nre库采用raw string类型表示正则表达式，表示为:r'text'\nraw string是不包含转义符的字符串\n-string类型，更繁琐。\n当正则表达式包含转义符时，使用raw string")]),t._v(" "),e("p",[t._v("3.re.search(pattern,string,flags=0)\n在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象\npattern：正则表达式的字符串或原生字符串表示\nstring:待匹配字符串\nflags:正则表达式使用时的控制标记\n-re.I re.IGNORECASE 忽略正则表达式的大小写，[A-Z]能够匹配小写字符\n-re.M re.MULTILINE 正则表达式中的^操作符能够将给定字符串的每行当作匹配开始\n-re.S re.DOTALL 正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符")]),t._v(" "),e("p",[t._v("4.re.match(pattern,string,falg=0)\n从一个字符串的开始位置起匹配正则表达式，返回match对象。\npattern：正则表达式的字符串或原生字符串表示\nstring:待匹配字符串\nflags:正则表达式使用时的控制标记")]),t._v(" "),e("p",[t._v("5.re.findall(pattern,string,flags=0)\n搜索字符串，以列表类型返回全部能匹配的子串。\npattern：正则表达式的字符串或原生字符串表示\nstring:待匹配字符串\nflags:正则表达式使用时的控制标记")]),t._v(" "),e("p",[t._v("6.re.split(pattern,string,maxsplit=0,flags=0)\n将一个字符串按照正则表达匹配结果进行分割，返回列表类型。\npattern：正则表达式的字符串或原生字符串表示\nstring:待匹配字符串\nmaxsplit:最大分割数，剩余部分作为最后一个元素输出。\nflags:正则表达式使用时的控制标记")]),t._v(" "),e("p",[t._v("7.re.finditer(pattern,string,flags=0)\n搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象。\npattern：正则表达式的字符串或原生字符串表示\nstring:待匹配字符串\nflags:正则表达式使用时的控制标记")]),t._v(" "),e("p",[t._v("8.re.sub(pattern,repl,string,count=0,flags=0)\n在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串。\npattern：正则表达式的字符串或原生字符串表示\nrepl:替换匹配字符串的字符串\nstring:待匹配字符串\ncount:匹配的最大替换次数\nflags:正则表达式使用时的控制标记")]),t._v(" "),e("p",[t._v("9.re库的另一种等价用法")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("rst=re.search(r'[1-9]\\d{5}','BIT 100081')\n函数式用法：一次性操作")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("pat=re.compile(r'[1-9]\\d{5}')\nrst=pat.search(\"BIT 100081\")")])])])]),t._v(" "),e("p",[t._v("10.regex=re.compile(pattern,flags=0)\n将正则表达式的字符串形式编译成正则表达式对象\npattern：正则表达式的字符串或原生字符串表示\nflags:正则表达式使用时的控制标记")]),t._v(" "),e("p",[t._v("##Re库的match对象")]),t._v(" "),e("p",[t._v("1.Match对象的属性\n.string 待匹配的文本\n.re 匹配时使用的pattern对象(正则表达式)\n.pos 正则表达式搜索文本的开始位置\n.endpos 正则表达式搜索文本的结束位置")]),t._v(" "),e("p",[t._v("2.Match对象的方法\n.group(0):获得匹配后的字符串\n.start():匹配字符串在原始字符串的开始位置\n.end():匹配字符串在原始字符串的结束位置\n.span():返回(.start(),.end())")]),t._v(" "),e("p",[t._v("##Re库的贪婪匹配和最小匹配")]),t._v(" "),e("p",[t._v("1.同时匹配长短不同的多项，返回哪一个呢？")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("match=re.search(r'PY.*N','PYANBNCNDN')\nmatch.group(0)")])])])]),t._v(" "),e("p",[t._v("2.贪婪匹配：Re库默认采用贪婪匹配，即输出匹配最长的子串")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("match=re.search(r'PY.*N','PYANBNCNDN')\nmatch.group(0)\nPYANBNCNDN")])])])]),t._v(" "),e("p",[t._v("3.最小匹配，如何输出最短的子串呢？")]),t._v(" "),e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("match=re.search(r'PY.*?N','PYANBNCNDN')\nmatch.group(0)\nPYAN")])])])]),t._v(" "),e("p",[t._v("4.最小匹配操作符\n*?前一个字符0次或无限次扩展，最小匹配\n+?前一个字符1次或无限次扩展，最小匹配\n??前一个字符0次或1次扩展，最小匹配\n{m,n}?扩展前一个字符m至n次(含n),最小匹配")]),t._v(" "),e("p",[t._v('#实例2：淘宝商品比价定向爬虫\n##"淘宝商品比价定向爬虫"实例介绍\n##"淘宝商品比价定向爬虫"实例编写')]),t._v(" "),e("p",[t._v('#实例3：股票数据定向爬虫\n##"股票数据定向爬虫"实例介绍\n##"股票数据定向爬虫"实例编写\n##"股票数据定向爬虫"实例优化')]),t._v(" "),e("p",[t._v("#网络爬虫之框架")]),t._v(" "),e("p",[t._v("#Scrapy爬虫框架\n##requests库和Scrapy爬虫的比较\n##Scrapy爬虫框架介绍\n##Scrapy爬虫框架解析\n##Scrapy爬虫的常用命令")]),t._v(" "),e("p",[t._v("#Scrapy爬虫基本使用\n##Scrapy爬虫的基本使用\n##Scrapy爬虫的第一个实例\n##yield关键字的使用")]),t._v(" "),e("p",[t._v('#股票数据Scrapy爬虫\n##"股票数据Scrapy爬虫"实例介绍\n##"股票数据Scrapy爬虫"实例编写\n##"股票数据Scrapy爬虫"实例优化')])])])],1)],1)],1)},[function(){var t=this.$createElement,n=this._self._c||t;return n("h1",{attrs:{id:"python语言开发工具选择"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#python语言开发工具选择","aria-hidden":"true"}},[this._v("#")]),this._v(" Python语言开发工具选择")])},function(){var t=this.$createElement,n=this._self._c||t;return n("h2",{attrs:{id:"requests库的安装"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#requests库的安装","aria-hidden":"true"}},[this._v("#")]),this._v(" Requests库的安装")])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\nr=requests.get(\"http://www.baidu.com\")\nr.status_code  #200\nr.encoding='utf-8'\nr.text\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("h2",{attrs:{id:"http协议及requests库方法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#http协议及requests库方法","aria-hidden":"true"}},[this._v("#")]),this._v(" HTTP协议及Requests库方法")])},function(){var t=this.$createElement,n=this._self._c||t;return n("h2",{attrs:{id:"requests库主要方法解析"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#requests库主要方法解析","aria-hidden":"true"}},[this._v("#")]),this._v(" Requests库主要方法解析")])},function(){var t=this,n=t.$createElement,e=t._self._c||n;return e("blockquote",[e("blockquote",[e("blockquote",[e("p",[t._v("kv={'key1':'value1','key2':'value2'}\nr=requests.request(\"GET\",'http://python123.io/ws',params=kv)\nprint(r.url)\nhttp://python123.io/ws?key1=value1&key2=value2\ndata:字典、字节序列或文件对象，作为Request的内容")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("kv={'key1':'value1','key2':'value2'}\nr=requests.request('POST','http://python123.io/wa',data=kv)\nbody='主体内容'\nr=requests.request('POST','http://python123.io/wa',data=kv)\njson:JSON格式的数据，作为Request的内容")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("kv={'key1':'value1'}\nr=requests.request('POST','http://python123.io/ws',json=kv)\nheaders:字典，HTTP定制头")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("hd={'user-agent':'Chrome/10'}\nr=requests.request('POST','http://python123.io/ws',headers=hd)\ncoolies:字典或CookieJar,Request中的cookie\nauth:元组，支持HTTP认证功能\nfiles：字典类型，传输文件")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("fs={'file':open('data.xls','rb')}\nr=requests.request('POST','http://python123.io/ws',files=fs)\ntimeout:设定超时时间，秒为单位")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("r=requests.request('GET','http://www.baidu.com',timeout=10)\nproxies:字典类型，设定访问代理服务器，可以增加登录认证")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("pxs={\n'http':'http://user:pass@10.10.10.1:1234'\n'https':'https://10.10.10.1:4321'\n}")])])]),t._v(" "),e("blockquote",[e("blockquote",[e("p",[t._v("r=requests.request('GET','http://www.baidu.com',proxies=pxs)\nallow_redirects:True/False,默认为True，重定向开关\nstream:True/False,默认为True，获取内容立即下载开关\nverify:True/False,默认为True，认证SSL证书开关\ncert:本地SSL证书路径")])])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("h2",{attrs:{id:"requests库的get-方法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#requests库的get-方法","aria-hidden":"true"}},[this._v("#")]),this._v(" Requests库的get()方法")])},function(){var t=this.$createElement,n=this._self._c||t;return n("blockquote",[n("blockquote",[n("blockquote",[n("p",[this._v('import requests\nr=requests.get("http://www.baidu.com")\nprint(r.status_code)\n200')])])]),this._v(" "),n("blockquote",[n("blockquote",[n("p",[this._v("type(r)\n<class 'requests.models.Response'>")])])]),this._v(" "),n("blockquote",[n("blockquote",[n("p",[this._v("r.headers\n{\n'Cache-Control':'private,no-cache,no-store,proxy-revalidate,ection':'keep-Alive','Transfer-Encoding':'chunked','Server':\n}")])])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("h2",{attrs:{id:"爬取网页的通用代码框架"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#爬取网页的通用代码框架","aria-hidden":"true"}},[this._v("#")]),this._v(" 爬取网页的通用代码框架")])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\ndef getHTMLText(url):\n\ttry:\n\t\tr=requests.get(url,timeout=30)\n\t\tr.raise_for_status()#如果状态不是200,引发HTTPError异常\n\t\tr.encoding=r.apparent_encoding\n\t\treturn r.text\n\texcept\n\t\treturn \"产生异常\"\nif __name__=='__main__':\n\turl='http://www.baidu.com'\n\tprint(getHTMLText(url))\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("p",[this._v("2.Robots协议基本语法\n#注释 "),n("em",[this._v("代表所有 /代表根目录\nUser-agent:")]),this._v("\nDisallow:/")])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\nurl='http://item.jd.com/2967795.html'\ntry:\n\tr=requests.get(url)\n\tr.raise_for_status()\n\tr.encoding=r.apparent_encoding\n\tprint(r.text[:1000])\nexcept:\n\tprint(\"爬取失败\")\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\nurl='http://www.amazon.cn/gp/product/B01M8L5Z3Y'\ntry:\n\tkv={'user-agent':'Mozilla/5.0'}\n\tr=requests.get(url,headers=kv)\n\tr.raise_for_status()\n\tr.encoding=r.apparent_encoding\n\tprint(r.text[1000:2000])\nexcept:\n\tprint(\"爬取失败\")\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\nkeyword='python'\ntry:\n\tkv={'q':keyword}\n\tr=requests.get(\"http://www.so.com/s\",params=kv)\n\tprint(r.request.url)\n\tr.raise_for_status()\n\tprint(len(r.text))\nexcept:\n\tprint(\"爬取失败\")\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\nimport os\nurl='http://image.nationalgeographic.com.cn/2017/0211/2545445.jpg'\nroot='D://pics//'\npath=root+url.split('/')[-1]\ntry:\n\tif not os.path.exists(root):\n\t\tos.mkdir(root)\n\tif not os.path.exists(path):\n\t\tr=requests.get(url)\n\t\twith open(path,'wb')as f:\n\t\t\tf.write(r.content)\n\t\t\tf.close()\n\t\t\tprint(\"文件保存成功\")\n\telse:\n\t\tprint(\"文件已存在\")\nexcept:\n\tprint(\"爬取失败\")\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[this._v("import requests\nurl='http://m.ip138.com/ip.asp?ip='\ntry:\n\tr=requests.get(url+'202.204.80.112')\n\tr.raise_for_status()\n\tr.encoding=r.apparent_encoding\n\tprint(r.text[-500:])\nexcept:\n\tprint(\"爬取失败\")\n")])])])},function(){var t=this.$createElement,n=this._self._c||t;return n("blockquote",[n("blockquote",[n("blockquote",[n("p",[this._v("from bs4 import BeautifulSoup\nsoup=BeautifulSoup('")]),n("html",[this._v("data")]),this._v("','html.parser')\nsoup2=BeautifulSoup(open('D://demo.html'),'html.parser')\nBeautifulSoup对应一个HTML/XML文档的全部内容"),n("p")])])])}],!1,null,null,null);n.default=s.exports}}]);